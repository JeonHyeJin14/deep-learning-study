# 1. 벡터 , 행렬 , 텐서
<img width="574" height="350" alt="image" src="https://github.com/user-attachments/assets/85ba2970-bbaa-49c8-b279-f7c44b38aa0a" />

- 1d-tensor (벡터) : 1차원으로 구성된 값
- 2d-tensor (행렬) : 2차원으로 구성된 값
- 3d-tensor (텐서) : 3차원으로 구성된 값
- 4차원이상부터는 머리로 생각하기가 어렵기 때문에 4차원은 3차원의 텐서를 위로 쌓아 올린 모습으로 상상해야함
- 5차원은 4차원을 확장, 6차원은 5차원을 확장

## 1) 차원
- 스칼라 : 숫자 1개 
- 벡터 : 숫자가 여러 개 나열 -> 1차원 텐서
- 행렬 : 행과 열 구조 -> 2차원 텐서

  ## 2) PyTorch Tensor Shape Convention
  ### a.2D Tensor : 행렬
<img width="394" height="300" alt="image" src="https://github.com/user-attachments/assets/a54aa8a1-f618-46ea-aba8-7479fb24fb69" />

- 행렬에서 행의 크기가 batch size, 열의 크기가 dim
- 
- ex) 벡터 하나에 숫자 256 길이, 이런 데이터가 3000개

  현재 데이터 크기는 3000*256 -> 행렬
  
  3000개를 한개씩 처리하기 보다는 덩어리로 처리
  
  만약 3000개에서 64개씩 처리한다고 하면 batch size를 64라고 함
  
  그렇다면 컴퓨터가 한 번에 처리하는 2D 텐서의 크기는 64*256

  총 47번 데이터를 넣어야 모든 데이터를 학습 할 수 있음

### b. 3D Tensor : 비전 분야에서의 3차원 텐서
<img width="405" height="308" alt="image" src="https://github.com/user-attachments/assets/fee778c6-ecb4-462d-854a-1adca35d3929" />

- 자연어 처리보다 비전 분야 (이미지, 영상 처리)를 하시게 된다면 좀 더 복잡한 텐서를 다루게됨
- 이미지는 가로 , 세로 존재 + 여러 장의 이미지 (batch size)로 구성하면 3차원 텐서 구조
- 세로 = batch size, 가로 = width, 높이 = height

### c. NLP 분야의 3D 텐서
<img width="382" height="273" alt="image" src="https://github.com/user-attachments/assets/4f13d71b-7374-4b8b-9575-0dd798019d76" />

- 자연어 처리는 보통 (batch size, 문장 길이, 단어 벡터의 차원)이라는 3차원 텐서를 사용
- 예제
```python
[[나는 사과를 좋아해], [나는 바나나를 좋아해], [나는 사과를 싫어해], [나는 바나나를 싫어해]]
```

아직 이 상태로는 '나는 사과를 좋아해'가 단어가 1개인지 3개인지 이해하지 못함

우선 컴퓨터의 입력으로 사용하기 위해서는 단어별로 나눠주어야 함

```python
[['나는', '사과를', '좋아해'], ['나는', '바나나를', '좋아해'], ['나는', '사과를', '싫어해'], ['나는', '바나나를', '싫어해']]
```

이제 훈련 데이터의 크기는 4 × 3의 크기를 가지는 2D 텐서 (행렬) 이다.

컴퓨터는 텍스트보다는 숫자를 더 잘 처리할 수 있기 때문에 각 단어를 벡터로 변형

```python
'나는' = [0.1, 0.2, 0.9]
'사과를' = [0.3, 0.5, 0.1]
'바나나를' = [0.3, 0.5, 0.2]
'좋아해' = [0.7, 0.6, 0.5]
'싫어해' = [0.5, 0.6, 0.7]

#훈련데이터를 재구성하면
[[[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.7, 0.6, 0.5]],
 [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.7, 0.6, 0.5]],
 [[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.5, 0.6, 0.7]],
 [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.5, 0.6, 0.7]]]
```

이제 훈련 데이터는 4 × 3 × 3의 크기를 가지는 3D 텐서이다. 이제 batch size를 2로 변형

```python
첫번째 배치 #1
[[[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.7, 0.6, 0.5]],
 [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.7, 0.6, 0.5]]]

두번째 배치 #2
[[[0.1, 0.2, 0.9], [0.3, 0.5, 0.1], [0.5, 0.6, 0.7]],
 [[0.1, 0.2, 0.9], [0.3, 0.5, 0.2], [0.5, 0.6, 0.7]]]
```

컴퓨터는 배치 단위로 가져가서 연산을 수행

그리고 현재 각 배치의 텐서의 크기는 (2 × 3 × 3) = (batch size, 문장 길이, 단어 벡터의 차원)의 크기입니다.


# 2.넘파이로 텐서 만들기 (벡터와 행렬 만들기)

```python
import numpy as np
```

## 1) 1D with Numpy
```python
t = np.array([0., 1., 2., 3., 4., 5., 6.])
# 파이썬으로 설명하면 List를 생성해서 np.array로 1차원 array로 변환함.
print(t)

#결과
[0. 1. 2. 3. 4. 5. 6.]


print('Rank of t: ', t.ndim)
print('Shape of t: ', t.shape)

#결과
Rank of t:  1
Shape of t:  (7,)
```

- .dim : 몇 차원인지 출력
- .shape : 크기를 출력 -> (7,) = 1*7의 크기를 가지는 벡터

### a. 넘파이 기초
```python
print('t[0] t[1] t[-1] = ', t[0], t[1], t[-1]) # 인덱스를 통한 원소 접근

#결과
t[0] t[1] t[-1] =  0.0 1.0 6.0
```
- 0번 인덱스를 가진 원소인 0.0, 1번 인덱스를 가진 원소인 1.0, -1번 인덱스를 가진 원소인 6.0이 출력되는 것을 보여줌
(-1번 인덱스는 맨 뒤에서부터 시작하는 인덱스)

### 슬라이싱 
- 범위 지정으로 원소 추출
- [시작 번호 : 끝 번호]
- 단 끝 번호는 포함하지 않음

```python
print('t[2:5] t[4:1] = ', t[2:5], t[4:1])

#결과
t[2:5] t[4:-1]  =  [2. 3. 4.] [4. 5.]
```
- [4:1] 은 4번 인덱스부터 끝에서 첫 번쨰 것까지의 결과를 가져온다는 의미

## 2) 2D with Numpy
```python
t=np.array([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.], [10., 11., 12.]])
print(t)

#결과
[[ 1.  2.  3.]
 [ 4.  5.  6.]
 [ 7.  8.  9.]
 [10. 11. 12.]]


print('Rank  of t: ', t.ndim) #차원 출력
print('Shape of t: ', t.shape) #행렬의 크기 출력

#결과
Rank  of t:  2 #2차원 = 행렬
Shape of t:  (4, 3) #4*3킈기
```

# 3. 파이토치 텐서 선언하기

```python
import torch
```

## 1) 1D with PyTorch
```python
t = torch.FloatTensor([0., 1., 2., 3., 4., 5., 6.])
print(t)

#결과
tensor([0., 1., 2., 3., 4., 5., 6.])

print(t.dim()) #rank=차원
print(t.shape()) #텐서 크기
print(t.size()) #텐서 크기

#결과
1 #1차원 텐서 = 벡터
torch.Size([7])
torch.Size([7]) #원소가 7개


print(t[0], t[1], t[-1])  # 인덱스로 접근
print(t[2:5], t[4:-1])    # 슬라이싱
print(t[:2], t[3:])       # 슬라이싱

#결과
tensor(0.) tensor(1.) tensor(6.)
tensor([2., 3., 4.]) tensor([4., 5.])
tensor([0., 1.]) tensor([3., 4., 5., 6.])
```

## 2) 2D with Pytorch
```python
t = torch.FloatTensor([[1., 2., 3.],
                       [4., 5., 6.],
                       [7., 8., 9.],
                       [10., 11., 12.]
                      ])
print(t)

#결과
tensor([[ 1.,  2.,  3.],
        [ 4.,  5.,  6.],
        [ 7.,  8.,  9.],
        [10., 11., 12.]])


print(t.dim())  # rank. 즉, 차원
print(t.size()) # shape

#결과
2 #2차원=행렬
torch.Size([4, 3]) #4*3 크기의 행렬

print(t[:, 1]) # 첫번째 차원을 전체 선택한 상황에서 두번째 차원의 첫번째 것만 가져온다.
print(t[:, 1].size()) # ↑ 위의 경우의 크기

#결과
tensor([ 2.,  5.,  8., 11.])
torch.Size([4])


print(t[:, :-1]) # 첫번째 차원을 전체 선택한 상황에서 두번째 차원에서는 맨 마지막에서 첫번째를 제외하고 다 가져온다.

#결과
tensor([[ 1.,  2.],
        [ 4.,  5.],
        [ 7.,  8.],
        [10., 11.]])
```
- 첫 번째 차원을 전체 선택한 상황에서 두 번째 차원에서는 맨 마지막에서 첫 번째를 제외하고 다 가져오는 경우


  ## 3) 브로드캐스팅
- 두 행렬 A, B가 있다고 할때 덧셈과 뺼셈을 하거나 곱셈을 하려면 차원의 제한이 있음
- 하지만 딥 러닝을 하게되면 불가피하게 크기가 다른 행렬 또는 텐서에 대해 연산을 수행해야함
- 파이토치에서는 자동으로 크기를 맞춰 연산을 수행하게 하는 브로드 캐스팅이라는 기능을 제공함

```python
m1 = torch.FloatTensor([[3, 3]])
m2 = torch.FloatTensor([[2, 2]])
print(m1 + m2)

#결과
tnesor([[5., 5.]])
```
m1, m2의 크기는 모두 (1,2) -> 연산에 문제가 없음

```python
# Vector + scalar
m1 = torch.FloatTensor([[1, 2]])
m2 = torch.FloatTensor([3]) # [3] -> [3, 3]
print(m1 + m2)

#결과
tensor([[4., 5.]])

```
원래는 연산이 불가능하지만 브로드캐스팅을 통해서 이를 연산 처리함
- m1의 크기는 (1,2)고 m2(1,) 크기 -> 파이토치는 (1,2)로 변경해 연산 수행

```python
# 2 x 1 Vector + 1 x 2 Vector
m1 = torch.FloatTensor([[1, 2]])
m2 = torch.FloatTensor([[3], [4]])
print(m1 + m2)

#결과
tensor([4., 5.],
       [5., 6.]])
```
m1의 크기는 (1,2) m2의 크기는 (2,1) -> 수학적으로 덧셈 불가 (곱셈은 가능)
파이토치는 두 벡터의 크기를 (2,2)로 변환하여 덧셈 수행

```python
# 브로드캐스팅 과정에서 실제로 두 텐서가 어떻게 변경되는지 보겠습니다.
[1, 2]
==> [[1, 2],
     [1, 2]]
[3]
[4]
==> [[3, 3],
     [4, 4]]
```
- 브로드 캐스팅은 편하지만 자동으로 실행되는 기능이므로 사용자 입장에서는 주의해서 사용해야함
- 두 텐서의 크기가 같다고 생각하여 연산했지만 실제로는 다른 경우 자동으로 브로드캐스팅이 수행되므로 원하는 결과가 나오지 않았더라도 어디서 문제가 발생했는지 찾기가 굉장히 어려움


## 4) 자주 사용되는 기능들

###1. 행렬 곱셈과 곱셈의 차이
- matmul : 행렬 곱셈
- mul : 곱셈

```python
m1 = torch.FloatTensor([[1, 2], [3, 4]])
m2 = torch.FloatTensor([[1], [2]])
print('Shape of Matrix 1: ', m1.shape) # 2 x 2
print('Shape of Matrix 2: ', m2.shape) # 2 x 1
print(m1.matmul(m2)) # 2 x 1


# 행렬곱 결과
ed!Shape of Matrix 1:  torch.Size([2, 2])
Shape of Matrix 2:  torch.Size([2, 1])
tensor([[ 5.],
        [11.]])

m1 = torch.FloatTensor([[1, 2], [3, 4]])
m2 = torch.FloatTensor([[1], [2]])
print('Shape of Matrix 1: ', m1.shape) # 2 x 2
print('Shape of Matrix 2: ', m2.shape) # 2 x 1
print(m1 * m2) # 2 x 2
print(m1.mul(m2))

#element-wise 곱셈 결과
Shape of Matrix 1:  torch.Size([2, 2])
Shape of Matrix 2:  torch.Size([2, 1]) -> 곱셈을 위해서 브로드캐스팅이 됨 -> 2*2 행렬로 변환
tensor([[1., 2.],
        [6., 8.]])
tensor([[1., 2.],
        [6., 8.]])
```

### 2) 평균 (MEAN)
```python
t = torch.FloatTensor([1, 2])
print(t.mean())

#result
tensor(1.5000)

#2*2 크기 행렬 평균
t = torch.FloatTensor([[1, 2], [3, 4]]) 
print(t)

#result
print(t.mean())

print(t.mean(dim=0))

#result
tensor([2., 3.])

print(t.mean(dim=1)) #dim=-1인자를 주는 경우와 같음

#result
tensor([1.5000, 3.5000])

```
- dim=0 : 첫 번째 차원을 의미함 (행) -> 행 방향으로 쭉 내려가면서 평균 → 열 기준 평균
- dim=1 : 두 번째 차원을 의미함 (열) ->  열 방향으로 쭉 가면서 평균 → 행 기준 평균

### 3) max와 argmax
- max : 원소의 최대값을 리턴
- argmax : 최대값을 가진 인덱스를 리턴

```python
t = torch.FloatTensor([[1, 2], [3, 4]])
print(t)

#result
tensor([[1., 2.],
        [3., 4.]])

print(t.max())

#result
tensor(4.)

print(t.max(dim-0)) #행을 제거 -> (1,2)짜리 텐서를 만든다.

#result
(tensor([3., 4.]), tensor([1, 1]))

