# 1. 데이터에 대한 이해 
- 예제 : 공부한 시간과 점수에 대한 상관관계

## 1) 훈련 데이터셋과 테스트 데이터셋
ex) 어떤 학생이 1시간 공부 -> 2점
다른 학생이 2시간 공부 -> 4점
다른 학생이 3시간 공부 -> 6점
Q. 내가 4시간을 공부한다면 몇 점을 맞을 수 있는지?

- 예측을 위해 사용하는 데이터 : 훈련 데이터셋
- 학습이 끝난 후 모델이 얼마나 잘 작동하는지 판별하는 데이터 : 테스트 데이터셋

## 2) 훈련 데이터셋의 구성
- 모델을 학습시키기 위한 데이터는 파이토치의 텐서의 형태 (torch.tensor)를 가지고 잇어야함
```python
# x_train은 공부한 시간, y_train은 그에 맵핑되는 점수
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[2], [4], [6]])
```

<img width="274" height="99" alt="image" src="https://github.com/user-attachments/assets/ff53f13f-6e24-4a06-ab73-b12aac6876e2" />

# 2. 가설 수립
- (머신러닝에서의) 가설 : 임의로 추측해서 세워보는 식 / 경헙적으로 알고 있는 식 -> 맞는 가설이 아니라고 판단되면 수정
- (선형 회귀의) 가설 : 
<img width="160" height="44" alt="image" src="https://github.com/user-attachments/assets/2f608514-a50d-4c57-aa24-63bcd05bb07d" />

<img width="221" height="49" alt="image" src="https://github.com/user-attachments/assets/09fad2eb-73e4-40c0-a15b-d52b3729cadb" />

- 이때 x와 곱해지는 W를 가중치, b를 편향이라고 한다.

# 3. 비용 함수
- 비용함수 = 손실함수= 오차함수 = 목적함수
- 모델이 예측한 값과 실제 정답 사이의 차이를 수치로 게산해주는 지표
- 이 함수의 값을 최소화 하는 것이 모델 학습의 목표
- 선형 회귀의 경우 손실함수는 평균제곱오차 (MSE)
- <img width="321" height="109" alt="image" src="https://github.com/user-attachments/assets/15f44027-3ce7-4a65-92ba-2dd0f52083a5" />

- 이진 분류의 경구 손실함수는 이진 크로스엔트로피
- <img width="496" height="48" alt="image" src="https://github.com/user-attachments/assets/4461f4f7-4f87-40c6-903c-70ab375d5663" />


# 4. 옵티마이저 - 경사 하강법 (Gradient Descent)
- 비용 함수의 값을 최소로 하는 가중치와 편향을 찾는 방법
- 옵티마이저 = 최적화 알고리즘
- 옵티마이저 알고리즘을 통해 적절한 가중치와 편향을 찾아내는 과정 = training
- 이번 설명에서는 편향을 고려하지않음 : y=Wx로 가정, 비용함수의 값 cost(W)는 cost라고 줄여서 표현

<img width="476" height="285" alt="image" src="https://github.com/user-attachments/assets/5c4a8ed3-31d2-47b7-a589-f40e3c57a805" />

- 가중치가 직선의 방정식에서는 기울기 -> W가 기울기

주황색 선은 기울기가 20, 초록색 선을 기울기가 1
- 각각 y=20x, y=x
- 기울기가 지나치게 크면 실제값과 예측값의 오차가 커지고, 기울기가 지나치게 작아도 실제값과 예측값의 오차가 커짐

<img width="285" height="217" alt="image" src="https://github.com/user-attachments/assets/eb5acca0-0fcf-4ada-9964-7ebecec4ff1a" />

- 기울기가 무한대로 커지거나 작아지면 cost의 값도 무한대로 커짐
- 위 그래프에서 cost가 가장 작을 때는 맨 아래의 볼록한 부분 -> 기계는 cost의 최소가 되는 가중치를 찾는 것

<img width="279" height="218" alt="image" src="https://github.com/user-attachments/assets/e5cfb681-22b6-403d-b461-37ab5df65658" />

- 기계는 임의의 초기값 가중치를 정하고 맨 아래의 볼록한 부분을 향해 점차 W값을 수정해나감
- 이를 가능하게 하는 것이 경사 하강법

<img width="278" height="259" alt="image" src="https://github.com/user-attachments/assets/f6224b8b-1e27-4d2c-9d80-32d19a2d2195" />

- 초록색 선은 W가 임의의 값을 가지게 되는 네 가지의 경우에 대해 그래프 상으로 접선의 기울기를 보여줌
- 맨 아래의 볼록한 부분으로 갈 수록 접선의 기울기가 점차 작아짐 -> 맨 아래의 볼록한 부분에서는 결국 접선의 기울기가 0이됨

- cost가 최소화가 되는 지점은 접선의 기울기가 0이 되는 지점이며, 미분 값이 0이 되는 지점입니다.
- Gradient descent의 아이디어는 비용 함수를 미분해 현재 가중치에서의 접선의 기울기를 구하고 기울기가 낮은 방향으로 W의 값을 변경하는 작업을 반복함
* gradient = 현재 W에 접선의 기울기
  
- <img width="309" height="93" alt="image" src="https://github.com/user-attachments/assets/9288912b-7ed1-45be-aa63-55d6eec24483" />

### 기울기가 음수일 때 : W읙 값이 증가함

<img width="645" height="59" alt="image" src="https://github.com/user-attachments/assets/eac23ead-13ef-43e9-a7c1-5762ff7770df" />

- 기울기가 음수면 가중치의 값이 증가하는데 이는 결과적으로 접선의 기울기가 0인 방향으로 W의 값이 조정됨

### 기울기가 양수일 때 : W의 값이 감소

<img width="360" height="49" alt="image" src="https://github.com/user-attachments/assets/1a51c040-419d-4a24-9aa0-968b82c2260b" />

- 기울기가 양수면 W의 값이 감소하게 되는데 이는 결과적으로 기울기가 0인 방향으로 W의 값이 조정됨
- 
<img width="350" height="96" alt="image" src="https://github.com/user-attachments/assets/41225bf6-961d-46e9-be78-9862ddcf7482" />

- 이 수식은 접선의 기울기가 음수거나, 양수일 때 모두 접선의 기울기가 0인 방향으로 W의 값이 조정함

### 학습률
- w의 값을 변경할때, 얼마나 크게 변경할지를 결정
- = W를 그래프의 한 점으로 보고 접선의 기울기가 0일 때까지 경사를 따라 내려간다는 관점에서는 얼마나 큰 폭으로 이동할지를 결정한다.
- 직관적으로 생각하기에 학습률의 값을 무작정 크게 하면 접선의 기울기가 최소값이 되는 가중치를 빠르게 할 수 있을 것 같지만 전혀 아님

<img width="278" height="229" alt="image" src="https://github.com/user-attachments/assets/0b4a0407-d9f4-4094-a690-9c6ac1be6e5d" />

- 학습률 a가 지나치게 높은 값을 가지면 접선의 기울기가 0 이되는 W를 찾아가는 것이 아니라 cost의 값이 발산하는 상황을 보여준다.
- 반대로 지나치게 낮은 값을 가지면 학습 속도가 느려지므로 적당한 a의 값을 찾아내는 것도 중요함

- 실제 경사 하강법은 W와 b에 대해 동시에 경사하강법을 수행하면서 최적의 W와 b의 값을 찾아나감

# 5. 파이토치로 구현하기
## 1) 기본 셋팅
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# 현재 실습하고 있는 파이썬 코드를 재실행해도 다음에도 같은 결과가 나오도록 랜덤 시드(random seed)를 줍니다.
torch.manual_seed(1)
```

## 2) 변수 선언 -> 훈련 데이터 셋
```python
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[2], [4], [6]])
```
x_train, y_train 모두 (3x1)크기

## 3) 가중치와 편향의 초기화
- 선형회귀 : 학습 데이터와 가장 잘 맞는 하나의 직선을 찾는 일
- 가장 잘 맞는 직선을 정의하는 것이 가중치와 편향
```python
# 가중치 W를 0으로 초기화하고 학습을 통해 값이 변경되는 변수임을 명시함.
W = torch.zeros(1, requires_grad=True) #requires_grad=True : 이 변수는 학습을 통해 계속 값이 변경되는 변수 의미
# 가중치 W를 출력
print(W) 

#result
tensor([0.], requires_grad=True)

b = torch.zeros(1, requires_grad=True)
print(b)

#result
tensor([0.], requires_grad=True)
```

현재 직선의 방정식 : y=0*x + 0

## 4) 가설 세우기
- 파이토치 코드 상으로 직선의 방정식에 해당되는 가설 선언
<img width="219" height="50" alt="image" src="https://github.com/user-attachments/assets/3ef4339e-668b-4901-80a6-aa45b5c28d98" />

```python
hypothesis = x_train * W + b
print(hypothesis)
```

## 5) 비용 함수 선언
- 파이토치 코드 상으로 선형 회귀의 비용함수에 해당되는 평균 제곱 오차를 선언
<img width="497" height="81" alt="image" src="https://github.com/user-attachments/assets/137a95a7-628b-4934-adc3-5db8f3fbd3f9" />

```python
# 앞서 배운 torch.mean으로 평균을 구한다.
cost = torch.mean((y_train-hypothesis)**2)
print(cost)

#result
tensor(18.6667, grad_fn=<MeanBackward0>)
# 교재는 0이 아니라 1이라고 나와있지만 , 연산 종류가 같아서 신경쓸 필요없음
```

## 6) 경사 하강법 구현 (Gradient decent)
### SGD : 경사 하강법의 일종
- 확률적 경사 하강법
- 전체 데이터가 아니라 무작위로 뽑은 일부 데이터 (미니배치)를 사용해서 빠르게 학습함


- Lr : 학습률

```python
# 모수 설정, 학습률 설정
optimizer = optim.SGD([W, b], lr=0.01)
```
optimizer.zero_grad() : 미분을 통해서 얻은 기울기를 0으로 초기화
- 초기화 하지 않으면 이전에 구한 기울기에 더해져 누적됨 -> 엉뚱한 방향으로 학습하거나 너무 큰 값으로 폭주
- 이전 기울기 초기화 -> 예측 -> 기울기 계산 -> 기울기 방향으로 가중치 업데이트

cost.backward() : 가중치와 편향에 대한 기울기 게산
optimizer.step() : 인수로 들어갔던 가중치와 편향에서 리턴되는 변수들의 기울기에 학습률을 곱해 빼줌 = 업데이트

```python
# gradient를 0으로 초기화
optimizer.zero_grad() 
# 비용 함수를 미분하여 gradient 계산
cost.backward() 
# W와 b를 업데이트
optimizer.step() 
```

## 7) 전체 코드
```python
# 데이터
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[2], [4], [6]])
# 모델 초기화
W = torch.zeros(1, requires_grad=True)
b = torch.zeros(1, requires_grad=True)
# optimizer 설정
optimizer = optim.SGD([W, b], lr=0.01)

nb_epochs = 1999 # 원하는만큼 경사 하강법을 반복
for epoch in range(nb_epochs + 1):

    # H(x) 계산
    hypothesis = x_train * W + b

    # cost 계산
    cost = torch.mean((hypothesis - y_train) ** 2)

    # cost로 H(x) 개선
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()

    # 100번마다 로그 출력
    if epoch % 100 == 0:
        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(
            epoch, nb_epochs, W.item(), b.item(), cost.item()
        ))
```
<img width="519" height="479" alt="image" src="https://github.com/user-attachments/assets/92ea66b7-aad2-44d5-a102-8ade76dffeec" />

- Epoch : 전체 훈련 데이터가 학습에 한 번 사용된 주기 -> 2000번 수행
- 최종 훈련 결과 최적의 기울기는 2에 가깝고 편향은 0에 가까움
- 실제 정답은 W가 2이고 b가 0인 2x이므로 거의 정답을 찾음
