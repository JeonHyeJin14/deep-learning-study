# 1. 미니 배치와 배치크기

<img width="771" height="202" alt="image" src="https://github.com/user-attachments/assets/207d5965-e1dd-4ff4-8cdd-58e1bd61bdd3" />

- 이 데이터의 샘플 수는 5개이고 전체 데이터를 하나의 행렬로 선언해 전체 데이터에 대해서 경사 하강법을 수행해 학습 할 수 있음
- 하지만 이 데이터는 현업에서 다루게 되는 방대한 양의 데이터에 비하면 굉장히 적은 양
-> 전체 데이터를 더 작은 단위로 나누어서 해당 단위로 학습하는 개념이 나오게 됨 = 미니 배치

<img width="616" height="341" alt="image" src="https://github.com/user-attachments/assets/34db065d-cd78-4688-816a-0a3444aa91a8" />

- 미니 배치 학습 : 미니 배치만큼만 가져가서 미니 배치에 대한 cost를 계산하고 경사 하강법을 수행 -> 다음 미니 배칠르 가져가서 수행 -> 마지막 미니 배치까지 수행
  -> 이 과정이 1 Epoch ( 전체 훈련 데이터가 학습헤 한 번 사용된 주기)

  결국 미니 배치의 개수는 배치의 크기를 몇으로 하느냐에 따라서 달라짐

  - 배치 경사 하강법은 전체 데이터를 사용하므로 가중치 값이 최적값에 수렴하는 과정이 매우 안정적이지만 계산량이 너무 많음
  - 미니 배치 경사 하강법은 일부 데이터만 수행하므로 최적값으로 수렴하는 과정에서 값이 조금 헤매기도 하지만 훈련 속도가 빠름
  - 배치 크기는 보통 2의 제곱수를 사용 (데이터의 송수신 효율성 때문에)

  # 2. 이터레이션 (Iteration)
- 한 번의 Epoch 내에서 이루어지는 매개변수인 가중치 W와 b의 업데이트 횟수
  
<img width="309" height="322" alt="image" src="https://github.com/user-attachments/assets/28e6fcec-64b5-4c6e-a81b-4f6917966e8d" />

- 전체 데이터가 2000일 때 배치 크기를 200으로 한다면 이터레이션수는 총 10개 -> 한 번의 Epoch당 매개변수 업데이트가 10번 이루어짐

# 3. 데이터 로드하기 (Data Load)

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

from torch.utils.data import TensorDataset # 텐서데이터셋
from torch.utils.data import DataLoader # 데이터로더

#데이터
from torch.utils.data import TensorDataset # 텐서데이터셋
from torch.utils.data import DataLoader # 데이터로더

#TensorDataset의 입력으로 사용하고 dataset으로 저장
dataset = TensorDataset(x_train, y_train)
