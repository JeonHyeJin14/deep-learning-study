<img width="776" height="262" alt="image" src="https://github.com/user-attachments/assets/5505c74f-6d2e-4844-bf96-41329f86ccca" /># 1. 데이터 이해
<img width="633" height="382" alt="image" src="https://github.com/user-attachments/assets/0c1d2de1-eb9f-4959-bf72-32ea1ea4c459" />

- 독립 변수의 개수가 3개 -> 3개의 퀴즈 점수로부터 최종 점수를 예측하는 모델 구현
- 
<img width="413" height="56" alt="image" src="https://github.com/user-attachments/assets/11831392-47c9-48d8-857f-0e8fdc3144f0" />

# 2. 파이토치로 구현
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

#시드고정
torch.manual_seed(1)
```

<img width="427" height="42" alt="image" src="https://github.com/user-attachments/assets/9414cbf9-fcfc-4fbf-994b-c67bf2750f68" />

```python
# 훈련 데이터
x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])
x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])
x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])
y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])

# 가중치 w와 편향 b 초기화
w1 = torch.zeros(1, requires_grad=True)
w2 = torch.zeros(1, requires_grad=True)
w3 = torch.zeros(1, requires_grad=True)
b = torch.zeros(1, requires_grad=True)

# optimizer 설정
optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)

#1000회 반
nb_epochs = 1000
for epoch in range(nb_epochs + 1):

    # H(x) 계산
    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b

    # cost 계산
    cost = torch.mean((hypothesis - y_train) ** 2)

    # cost로 H(x) 개선
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()

    # 100번마다 로그 출력
    if epoch % 100 == 0:
        print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(
            epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()
        ))
```

<img width="776" height="262" alt="image" src="https://github.com/user-attachments/assets/417d2ade-3a9e-4c82-a7b7-f1609cc22feb" />

- hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b에서도 x_train의 개수만큼 w와 곱해줌

# 3. 벡터와 행렬 연산으로 바꾸기
- 위의 경우 x의 개수대로 가중치도 일일히 선언함 --> x의 개수가 많아지면 불가능
- 이를 해결하기 위해서 행렬 곱셈 또는 벡터의 내적을 사용

<img width="612" height="246" alt="image" src="https://github.com/user-attachments/assets/be270a43-1244-4bf4-87ca-bb617c42ff11" />

## 1) 벡터 연산으로 이해
<img width="380" height="64" alt="image" src="https://github.com/user-attachments/assets/bc5cf277-4c02-48b9-b2e1-b232f2391c64" />

이 식을 벡터의 내적으로 표현하면 

<img width="436" height="87" alt="image" src="https://github.com/user-attachments/assets/f98747b3-04f4-4f4c-80ed-88b251ee0e9e" />

## 2) 행렬 연산으로 이해
<img width="617" height="400" alt="image" src="https://github.com/user-attachments/assets/ebe2ee8b-fe4a-4cf9-a91e-e1ec1fa82281" />

- 전체 훈련 데이터의 개수를 셀 수 있는 1개의 단위를 샘플이라고하고, 현재 샘플 수는 5개
- 각 샘플에서 y를 결정하게 하는 각각의 독립변수 x를 특성이라고 하고, 현재 특성은 3개
- 즉, 독립 변수 x들의 수가 ( 샘플 수 * 특성의 수) = 15개

<img width="692" height="223" alt="image" src="https://github.com/user-attachments/assets/c949ccce-9c3d-473a-821e-47da39fe982b" />

<img width="184" height="55" alt="image" src="https://github.com/user-attachments/assets/9a1e5942-a80b-4096-a7ba-57eb3139668e" />

이 가설에 각 샘플에 더해지는 편향b를 추가

<img width="829" height="215" alt="image" src="https://github.com/user-attachments/assets/aa754285-162d-488f-8955-5212b9ab7a31" />
<img width="267" height="56" alt="image" src="https://github.com/user-attachments/assets/ede2977c-996a-4063-8bac-089c3ea1ae84" />

- 결과적으로 전체 훈련 데이터의 가설 연산을 3개의 변수만으로 표현함
- 벡터와 행렬 연산은 식을 간단하게 해줄 뿐 아니라 다수의 샘플 병렬 연산이기에 속도도 빠름

# 4. 행렬 연산을 고려해 파이토치로 구현
```pyton
x_train  =  torch.FloatTensor([[73,  80,  75], 
                               [93,  88,  93], 
                               [89,  91,  80], 
                               [96,  98,  100],   
                               [73,  66,  70]])  
y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])

# x는 ( 5x3 ) 행렬 = X

# 가중치와 편향 선언
W = torch.zeros((3, 1), requires_grad=True) # 가중치 W의 크기가 (3,1) 벡터
# x_train의 크기가 5*3 이므로 가중치 벡터의 크기가 3*1 이어야 행렬곱이 가능함
b = torch.zeros(1, requires_grad=True)

# 행렬곱으로 가설 선언 : .matmul
hypothesis = x_train.matmul(W) + b
```

# 5. 전체 코드
```pyton
x_train  =  torch.FloatTensor([[73,  80,  75], 
                               [93,  88,  93], 
                               [89,  91,  80], 
                               [96,  98,  100],   
                               [73,  66,  70]])  
y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])

# 모델 초기화
W = torch.zeros((3, 1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)
# optimizer 설정
optimizer = optim.SGD([W, b], lr=1e-5)

nb_epochs = 20
for epoch in range(nb_epochs + 1):

    # H(x) 계산
    # 편향 b는 브로드 캐스팅되어 각 샘플에 더해집니다.
    hypothesis = x_train.matmul(W) + b

    # cost 계산
    cost = torch.mean((hypothesis - y_train) ** 2)

    # cost로 H(x) 개선
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()

    print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(
        epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()
    ))
```

예측
```pyton
# 임의의 입력 값에 대한 예측
with torch.no_grad():
    new_input = torch.FloatTensor([[75, 85, 72]])  # 예측하고 싶은 임의의 입력
    prediction = new_input.matmul(W) + b
    print('Predicted value for input {}: {}'.format(new_input.squeeze().tolist(), prediction.item()))
```
<img width="647" height="35" alt="image" src="https://github.com/user-attachments/assets/6ebce3e8-c0cf-471d-9b5b-25aef1acbce9" />

- with torch.no_grad() : 블록 안에서 수행되는 모든 연산에 대해 기울기 계산을 비활성화 -> 예측을 할 때는 가중치 업데이트가 불필요
- new_input = torch.Floattensor : 예측하고자 하는 새로운 입력값 정의 , 각 숫자는 특정 피쳐
- prediction = new_input.matul(W) + b : 모델이 새로운 입력에 대해 예측한 값을 계산 -> 이때 W와 b는 학습 과정을 통해 얻어진 최적의 값

- SGD 기법으로 W와 b값을 업데이트 시키기 때문에 Epoch 마다 cost값은 줄어든다 -> 결국 최종 Epoch 의 가중치와 편향값이 최적화 값
- 하지만 너무 많이 돌리면 과적합
